---
title: "Class 5 notes and code"
output: html_document
---



$\\$




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)

knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview

 * Quick review
 * Sampling distributions continued
 * Confidence intervals
 * The bootstrap
 * Using the bootstrap to calculate confidence intervals in R




![](https://raw.githubusercontent.com/emeyers/SDS230_F19/master/class_images/class_05/broccoli.png)




$\\$





## Part 1: Review


$\\$




#### Part 1.1: Probability density functions 


$\\$


Probability density functions can be used to model random events. All **probability density functions**, *f(x)*, have these properties:

1. The function are always non-negative
2. The area under the function integrates (sums) to 1


For continuous (quantitative) data, we use density function f(x) to find the probability (e.g., model of the long run frequency) that a random outcome X is between two values *a* and *b* using:


$P(a < X < b) = \int_{a}^{b}f(x)dx$


![](https://raw.githubusercontent.com/emeyers/SDS230_F19/master/class_images/class_04/area_pdf.gif)




Probability density functions are a way to describe the (potentially infinite) population data (think Plato). Population density functions typically have *parameters* which we can estimate from by calculating *statistics* from samples of data.

Note: the term **probability distribution** is a term that can refer to:

1. the *probability density function* (or the probability mass function for descrete data)
2. the *cumulative distribution function* which is the integral (sum) of the density function

$$P(X < x) = F(x) = \int_{y < x}f(y)dy$$



$\\$




##### Example 1: The normal distribution 

The normal distribution has two parameters that define the distribution:

  1. $\mu$: which is the center of the distribution
  2. $\sigma$: which is the spread of the distribution 
  
  
The equation for the normal density function is:

$f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$


A given set of values for $\mu$ and $\sigma$ definite a specific normal distribution from the larger family of all possible normal distributions.

Below we plot a normal distributino for $\mu$ = 100 and $\sigma$ = 15


```{r normal_density}

x <- seq(30, 180, by = .001)  # a range of x values
y <- dnorm(x, 100, 15)  # the density function at each value of x

# plot the probability density function 
plot(x, y, 
     type = 'l', 
     ylab = "f(x)", 
     main = TeX("Normal density with $\\mu$ = 100, $\\sigma$ = 15"))


# we can get random points from this distribution using the rnorm function
n <- 20
(rand_data <- rnorm(n, 100, 15))

```


Important facts about the normal distribuition: 
 
 * 68% of the probability mass is within 1 $\sigma$ of $\mu$
 * 95% of the probability mass is within 2 $\sigma$ of $\mu$
 * 99.7% of the probability mass is within 3 $\sigma$ of $\mu$


![](https://raw.githubusercontent.com/emeyers/SDS230_F19/master/class_images/class_05/normal-distrubution-3sdspng.png)



![](https://raw.githubusercontent.com/emeyers/SDS230_F19/master/class_images/class_05/normal_pillow.jpg)


$\\$




##### Example 2: The exponential distribution

The exponential distribution has one parameter that defines the distribution 

  1. $\lambda$: the rate parameter  ($\lambda > 0$)
  
  
The equation for the exponential density function is:

$f(x | \lambda) = \lambda e^{-\lambda x}$, for $x \ge 0$, and 0 otherwise.


You will explore this distribution more and sampling distributions derived from it on the homework.  


*Bonus exercise*: Use calculus to show this is a valid probability density function (i.e, it is non-negative and integrates to 1). 




$\\$




#### Part 1.2: The law of large numbers




$\\$



The *law of large numbers* states that as the sample size *n* grows the sample average approaches the expected value (i.e., the parameter $\mu$). We can write this as $n \rightarrow \infty$  $\bar{x} \rightarrow \mu$. 


This is a nice theoretical property that our statistic converges to our parameter value (i.e., that our statistic is consistent), but we live in the *real world* where we have finite data. What is of real interest is if we have a finite sample size *n* what does the *distribution of statistics* we get look like, i.e., what does the **sampling distribtuion** look like? 




$\\$




## Part 1.3: Sampling distributions


A distribution of statistics is called a **sampling distribution** 


```{r}

sampling_dist <- NULL

# create a sampling distribution of the mean using data from a uniform distribution
for (i in 1:1000){
  rand_sample <- runif(100) 
  sampling_dist[i] <- mean(rand_sample)
}


# plot a histogram of the sampling distribution of these means 
hist(sampling_dist, nclass = 100,
     xlab = bquote(bar(x)), 
     main = "Sampling distribution of the sample mean")


```


For many statistics (and the sample average $\bar{x}$ in particular), our sampling distribution is often normal. Any ideas why this is? 

We will discuss ways to visualize whether data comes from a particular distribution (and possibly also hypothesis tests as well), later in the semester. 



$\\$




#### Part 1.3: The standard error (SE)

The deviation of a sampling distribution is called the standard error (SE). Can you calculate the standard error for the sampling distribution you created above? 

```{r}

# caclulate the SE as the standard deviation of the sampling distribution
sd(sampling_dist)

```






$\\$






## Part 2:  The central limit theorm

The [central limit theorm (CTL)](https://en.wikipedia.org/wiki/Central_limit_theorem) establishes that (in most situations) when independent random variables are added their (normalized) **sum** converges to a normal distribution.

Put another way, if we definte the average random (i.i.d) sample {$X_1$, $X_2$, ..., $X_n$} of size *n* as: 

$S_{n}:={\frac{X_{1}+\cdots +X_{n}}{n}}$

then the CTL tells us that:

$\sqrt{n}(S_{n} - \mu)$ $\xrightarrow {d} N(0,\sigma^{2})$


You will explore this more through simulations on homework 2. 





$\\$





## Part 3: Confidence intervals


$\\$


Confidence intervals are ranges of values that capture a parameter a fix proportion of time.

Let's explore this more...



$\\$






## Part 4: The bootstrap

The bootstrap is a method that can be used to estimate confidence intervals for a large range of parameters. 

The central concept behind the bootstrap is the "plug-in principle" where we treat our sample of data as if it were the population. We then sample *with replacement* from out sample to create a *bootstrap distribution* which is a proxy for (the spread of) the sampling distribution. 



$\\$



#### Part 4.1: Creating a bootstrap distribution in R


To sample data in R we can use the `sample(the_data)` function. To sample data with replacement we use the replace = TRUE argument, i.e., `sample(the_data, replace = TRUE)`.  

Below we calculate the bootstrap distribution for mean age of OkCupid users using just the first 20 OkCupid users in the data set. 


```{r}

library(okcupiddata)

# get the ages from the first 20 OkCupid profiles

# create the bootstrap distribution
bootstrap_dist <- NULL



# plot the bootstrap distribution to make sure it looks normal




```


$\\$



#### Part 4.2: Calculating the bootstrap standard error SE*


The standard deviation of the *bootstrap distribution* is usually a good approximation of the standard deviation of the sampling distrubition - i.e, it is a good approximation of the *standard error SE*.

When our bootstrap distribution is relatively normal, we can use the fact that 95% of values fall within to standard deviations of a normal distribution to calculate 95% confidence intervals as:

$CI_{95} = [stat - 2 \cdot SE^*,  stat + 2 \cdot SE^*]$

For example, for a our bootstrap distribution we have a 95% confidence interval for the mean $\mu$ as: 

$CI_{95} = [\bar{x} - 2 \cdot SE^*,  \bar{x} + 2 \cdot SE^*]$




```{r bootstrap_SE}

# calculate the bootstrap standard error SE* as the standard deviation of the bootstrap distribution 



# calculate the 95% CI using SE*




```



Above we are using the bootstrap to create a 95% confidence interval which should capture the mean age $\mu_{age}$


**Question:** What is the population that $\mu$ represents? 

 a) The data set (data frame) of 59,946 cases?
 b) All OkCupid users? 
 c) All US citizens?
 d) All adults in the world?

 
It is important to define what the larger population is when doing statistical inference since you are really only answering questions about this population. 
















