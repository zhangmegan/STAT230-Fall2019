---
title: "Class 22 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$





```{r setup, include=FALSE}
# install.packages("latex2exp")
library(latex2exp)
#options(scipen=999)
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
```



$\\$




## Overview: Multiple linear regression continued



 * Comparing models with R^2, adjusted R^2, AIC and BIC
 * Cross-validation
 * Plotting using grammar of graphics with ggplot
 * Bonus ggplot features






$\\$







## Part 1: Comparing models with R^2, adjusted R^2, AIC, BIC and cross-validation


As we discussed, we can compare models using several measures including $R^2$, $R^2_{adj}$, $AIC$, $BIC$ and cross-validation. Let's try this out on the baseball data.




$\\$





#### Part 1.1: Comparing models with R^2, adjusted R^2, AIC, BIC


Let's compare our predictions of runs in baseball using the model with the original variables and the model that also includes the variables that includes statistics normalized by at bats.




```{r}
library(Lahman)
library(dplyr)
# reduce the data frame to the main batting statistics of interest
team_batting2 <- select(Teams, yearID, teamID, G,  W,  L, R,  AB,  H,  X2B, X3B,  HR,  BB,  SO, SB, CS,  HBP, SF) %>%
  mutate(X1B = H - (X2B + X3B + HR),
         X1Bn = X1B/AB, 
         X2Bn = X2B/AB, 
         X3Bn = X3B/AB, 
         XHRn = HR/AB, 
         XBBn = BB/AB)                        
fit1 <- lm(R ~ X1B + X2B + X3B + HR + BB + AB, data = team_batting2)
fit2 <- lm(R ~ X1B + X2B + X3B + HR + BB + X1Bn + X2Bn + X3Bn + XHRn + XBBn, data = team_batting2)
fit1_summary <- summary(fit1)
fit2_summary <- summary(fit2)
cat('R^2 \n')
fit1_summary$r.squared
fit2_summary$r.squared   # the more complex model always has as higher R^2
cat("\n Adjusted R^2 \n")
# compare adjusted R^2 
fit1_summary$adj.r.squared
fit2_summary$adj.r.squared  # the more complex model has as higher adjusted R^2
cat("\n AIC \n")
AIC(fit1)
AIC(fit2)  # model 2 has as lower AIC so it is preferred
cat("\n BIC \n")
BIC(fit1)
BIC(fit2)  # model 2 has as lower BIC so it is preferred
```






$\\$






#### Part 1.2: Cross-validation


Let's now compare the models using cross-validation. To keep things simple, we are just going to split the data once into a training set and a test set rather than do k-fold cross-validation. 

We will compare the models based on their mean squared prediction error (MSPE) where a smaller MSPE is better. 



```{r}
# create the training set and the test set
total_num_points <- dim(team_batting2)[1]
num_training_points <- floor(total_num_points/2)
training_data <- team_batting2[1:num_training_points, ]
test_data <- team_batting2[(num_training_points + 1):total_num_points, ]
# fit both models on the training data, and calculate the MSPE based on thier predictions on the test set
fit_cv_1 <- lm(R ~ X1B + X2B + X3B + HR + BB + AB, data = training_data)
test_predictions_1 <- predict(fit_cv_1, newdata = test_data)
MSPE_smaller_model <- mean((test_data$R - test_predictions_1)^2)
  
fit_cv_2 <- lm(R ~ X1B + X2B + X3B + HR + BB + X1Bn + X2Bn + X3Bn + XHRn + XBBn, data = training_data)
test_predictions_2 <- predict(fit_cv_2, newdata = test_data)
MSPE_larger_model <- mean((test_data$R - test_predictions_2)^2)
MSPE_smaller_model   # smaller MSPE is better
MSPE_larger_model
```







$\\$

